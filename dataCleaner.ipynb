{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaD2tfgPXQek9pZ+18Tx80"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kI3p8w53XwC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Custom Imputer for handling missing values with specified strategies\n",
        "class CustomImputer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, strategy_dict=None):\n",
        "        self.strategy_dict = strategy_dict or {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.impute_models = {}\n",
        "        for column, strategy in self.strategy_dict.items():\n",
        "            if strategy in ['mean', 'median', 'most_frequent']:\n",
        "                self.impute_models[column] = SimpleImputer(strategy=strategy)\n",
        "                self.impute_models[column].fit(X[[column]])\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        for column, model in self.impute_models.items():\n",
        "            X[column] = model.transform(X[[column]])\n",
        "        return X\n",
        "\n",
        "# OutlierHandler for detecting and handling outliers\n",
        "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, contamination=0.01):\n",
        "        self.contamination = contamination\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.model = IsolationForest(contamination=self.contamination)\n",
        "        self.model.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        outliers = self.model.predict(X) == -1\n",
        "        return X[~outliers]\n",
        "\n",
        "# Data visualization function for missing values\n",
        "def plot_missing_values(df):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "    plt.title('Missing Values Heatmap')\n",
        "    plt.show()\n",
        "\n",
        "# Main cleaning function\n",
        "def clean_dataset(file_path, cleaned_file_path, strategy_dict=None, remove_outliers=True):\n",
        "    df = pd.read_csv(file_path)\n",
        "    plot_missing_values(df)\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', CustomImputer(strategy_dict=strategy_dict))\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    cleaning_steps = [('preprocessor', preprocessor)]\n",
        "\n",
        "    if remove_outliers:\n",
        "        cleaning_steps.append(('outliers', OutlierHandler()))\n",
        "\n",
        "    cleaning_pipeline = Pipeline(steps=cleaning_steps)\n",
        "    df_cleaned = cleaning_pipeline.fit_transform(df)\n",
        "    df_cleaned = pd.DataFrame(df_cleaned, columns=numeric_cols.append(categorical_cols))  # Adjust as needed based on OneHotEncoder output\n",
        "\n",
        "    df_cleaned.to_csv(cleaned_file_path, index=False)\n",
        "    print(\"Dataset cleaned and saved to:\", cleaned_file_path)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    strategy_dict = {'Age': 'mean', 'Salary': 'median'}\n",
        "    clean_dataset(\"path/to/your/dataset.csv\", \"path/to/your/cleaned_dataset.csv\", strategy_dict=strategy_dict, remove_outliers=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes for Deployment:\n",
        "\n",
        "Customization Required: Before deployment, review and customize the clean_dataset function, especially the handling of numeric and categorical columns, to match your dataset's specifics.\n",
        "\n",
        "Strategy Dictionary: The strategy_dict parameter in CustomImputer allows specifying different imputation strategies for different columns. Adjust this as needed based on your dataset's characteristics.\n",
        "\n",
        "Outlier Removal: The OutlierHandler uses Isolation Forest for outlier detection and removal. The contamination parameter can be adjusted based on the expected proportion of outliers in your dataset.\n",
        "\n",
        "Data Visualization: The plot_missing_values function provides a visual overview of missing data. This step can be skipped or enhanced with additional visualizations for a more in-depth analysis.\n",
        "\n",
        "Parallel Processing: While the code is designed for efficiency, further optimizations using parallel processing (e.g., with joblib) could be explored for handling very large datasets."
      ],
      "metadata": {
        "id": "zWH6CFrl7PnE"
      }
    }
  ]
}